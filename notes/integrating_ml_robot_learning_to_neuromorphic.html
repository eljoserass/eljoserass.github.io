<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>ml robot learning to neuromorphic</title>
  <link rel="stylesheet" href="/style.css">
</head>
<body>
  <h1>integrating ml robot learning to neuromorphic</h1>
<p>the issue is that noone trains with neuromorphic stack. it is not to blame, is not near as common to gpu based stacks, making hardware price and overall performance less competitve</p>
<p>anyway</p>
<p>the plan is to ask how can we use the exisiting amazing infrastructure for ml to build with the neuromorhphic stack, after all they are both ai learning so its not like completely different</p>
<p>we need to leverage real world data and simulation data, both of which are not necesarily well suited for the data formats we need. but thats it, is mostly data formats so here we go</p>
<p>for supervised learning, we will leverage lerobot dataset format which will enable us to use a wide range of robotic data from the community with the soarm, and many other robots that are adopting it. this framework makes it easy to map observation to supervised motor actions. </p>
<p>what we need to is make a easy way to transform the rgb input to the sparse, high frequency nature of event based camera. for this we have simulators so initally would be to plugin a module that would enable to rotate among the various simulators. the module will be fed the episode and will return the sequence of events from the simulated camera.</p>
<p>another challenge we might encounter is how to match the temporal resolution of the annotations. if the dataset was originally collected by mapping a 30fps camera feed, each frame will have a set of joint angels, now if we extend that instead that having 30 frames of visual information in a second to twice or five times as that (have to check what would be a fair comparison) there will be the challenge on how to smootly map the events to the joints at each timestep. it shouldnt be that difficult to find a solution tho, the temporal resolution of the visual intput doesnt have to be the same that the one in motor joints. an analogy would be that i can see way faster how and where a ball is moving than to actually process that i have to move there and activating my muscles to go there.</p>
<p>ok so now we succesfully translated lerobot format of rgb and action of angles position to a neuromorphic format, we  pretrained our snn and now want to do some RL, for this we need to have the same format on simulation</p>
<p>a possible implementation would be to create plugin (OmniGraph?) or add a event based camera sensor in Isaac Sim,  and burn some gpu to be able to preprocess the input from isaac sim into events with the temporal resolution required and trained an snn to complete the actions. It might be an option to use the lerobot dataset format used in the previous step but im not sure if there is something like lerobot dataset in the loop training.</p>
<p>now in the ideal case, our network learned something meaningful and is able to at least do the cool things that people do with rgb VLAs but with event + snns (could be a transformer snn but maybe that is not super interesting nor as good)</p>
<p>we now want to deploy it, using a gpu would be super slow and defeats the purpose of the architecture we are using, it wont have the benefit of low power, aync, parallel computation.</p>
<p>we could use something like this other project, nir-to-fpga, which transform the snn torch (or whatever other framework that is compatible used in training) to basic primitives that can be used a ground truth translate to the hardware. this software will in a way compile the graph into an HDL that we can synthetize in the fpga. the challenge for deployment might also be looking on how we can interface the cameras and the motors with the primitives that we translated.</p>
<p>also, would be cooler to use a real event camera, this might be expensive and comes with its own challegenges so another option could be to use the soarm101 platform as is. if we go this path we would then need to also include into our compute module the translation mechanism from rgb to events, it shouldnt add that much latency but we will need to take into consideration the temporal resolution of the events would be bottlenecked by the fps of the camera we are using. we might able to simulate the events in between the frames but the real temporal resolution will be the same as the rgb camera</p>
<p>but why all this effort tho? if vlas with nvidia chips are killing it? good question, i have yet to create a good answer but i think is something like this;</p>
<p>to fully exploit the potential benefits of neuromorphic computing, we need to start creating the infrastructure that increases the iteration speed of hypothesis -&gt; experiments -&gt; results. it is tricky because this cycle itself is needed for the infrastructure, so at first it is more difficult but it will become easier as the field progresses. </p>
<p>and what are these potential benefits? for me the indication that the energy of the intelligence blob is consumed in a complete different way is enough to start digging, not only because of the inmediate benefits that we could slash energy consumption of current systems by 100x, which is key in cloud computing and edge computing (although on the edge the motos spend way more energy that the computer, varies accross robot fingerprint) but for the potential benefits that this insight could yield, at the end we are closely listening to what nature did to replicate it better. that is how machine learning started, everyone was betting on symbolic systems, that maybe on some analogous level they were inspired by nature, but dismissed neural nets as just a cool thing that does not work. i think current state of research is like that. we are using neural nets but the state of the art is just a bunch of manual trial and error and combining things that work and putting llms to everything pretty much</p>
<p>there is some evidence that snn can perform well, although not that substantial. another benefit which we might be able to exploit more in the future is robustness. a LIF acts a a moving average, dismissing edge cases over time (noise) and integrating only more common atributes (info) (maybe idk) this might be very importnat when we need robots deployed for longs period of time in extreme situations. also, another case of noise robustness, maybe less substantial is that i think robustness can be a sign of generalization. a general system would understand the general patterns of a state, the fundamental concepts behind the observations, and when presented with new noisy observations would be able to robustly continue operation as it understood the reality behind them, acting more on a latent space rather than the light changes or force sensing. (this also goes to hand with JEPAs which is something i would like to integrate with neuromorphic in the future) </p>
<p>beyond these, as said previously im sure that listening closely to the spikes in our brain rather than curve fitting, would lead to create systems that have properties that are nowhere near possible with systems today. and once we now how to build a brain, and understand intelligence very well, it would be trivial to tune it to our needs, and turn the IQ knob to the roof and create machine gods.</p>
<p>until then, im more interested in understanding "simpler" brains, that clearly show intelligence,  for instance, everyones favorite ecosystem engineer <strong>the beaver!</strong> (more to why the beaver in particular later)</p>
<p>future research beyond just trying to match gradient based transformers (which we will!) is understanding local learning rules, or replicating more complex topologies in the brain rather than just feedforward. how can it communicate in every direction, every time? how different regions communicate with eachother? how does this create any meaning at all? 
you have recursions, feedworward, lateral, convolutions, communication bettween feedforward regions. all asynchrousnly, no one waiting for nobody. and at the same time everyone synchornyses in a way that they can collaborate which eachother and learn how to do something useful in any new enviroments. and now only that but how it encodes intelligenc in the dna? like how a zebra is born walking right a way?  how evolution by natural selection trained the synapses, firing thresholds, topologies, neurotransmitters and everything else the brain has?</p>
<p>the beauty about neuromorhpic computing is this, that you are always paying attention at biology, to create a computational model of how it creates miracles, train it to something useful, design a chip that behaves like the brain. </p>
<p>we need intelligence to need expanding, the universe wants to be perceived, we will create healthier animals, expand them with hardware, and give birth to ones that come from metal.</p>
</body>
</html>